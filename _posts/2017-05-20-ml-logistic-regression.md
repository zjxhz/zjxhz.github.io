---
layout: post
title: 逻辑回归
description: 用类似于预测房价的办法给美女分类的理论
category: articles
tags: 
---
在介绍完了近乎玩具的近邻分类法之后，有必要介绍一个真正的机器学习算法了。这个方法就叫「逻辑回归」（Logistic Regression）。之前我们房价预测的那个算法叫「线性回归」。这个之所以叫这个名字，可能因为结果只有两个：是或否（所以更严格的讲，是二元逻辑回归，其实可以有更多的分类）。而房价预测的结果则是一个实数，即价格。

## 为什么不用「线性回归」？
能不能用类似房价预测的方法来给人脸分类呢？如果我们回顾一下，房价和面积的关系被定义为：

$$ h(X) = \theta_0 +\theta_1 X $$

这里，X为面积，而h(X)为价格。类似的，一张50x50的人脸图片由2500个数字组成，那我们是不是可以用同样的方式来表示呢？即：

$$ h(X) = \theta_0 +\theta_1 X_1 + \theta_2 X_2 + ... + \theta_{2500} X_{2500} $$

向量化的简写则为：

$$ h(X) = \theta^T X $$

如果我们把$$\theta$$初始化为一个(-1,1)之间的数组，那么，对于其中的部分人脸，$$h(X)$$，也就是y的取值范围大致如下图所示：
![ml-lr-theta-minus-one-and-one.png](/images/ml-lr-theta-minus-one-and-one.png)
其中，绿色的点表示美女，红色的表示不是。注意：x轴的取值只代表是第i个训练样本，跟图片的像素值没有关系。因为这是一个高维数组，很难用图来表示。

可以看到，取值大约在-2000到5000之间。取值范围这么大的数字，如何对应是否美女呢？有一个简单地办法是，我们可以宣称，大于0则是，小于0则否。然后可以按照类似预测房价的方式，利用例如「梯度下降法」找到一组最优的参数，即$$\theta$$数组。这么做行得通吗？

让我们看一个更为简单的例子（取自Ng教授公开课），即根据肿瘤的大小来预测是良性还是恶性。这样，我们的样本就只有一个变量，即肿瘤大小。如图：
![ml-lr-ng0.png](/images/ml-lr-ng0.png)

为此，我们可以用线性回归的办法，画出一条预测曲线，并声称当y大于0.5的时候，肿瘤是恶性的（y的取值称为「决策边界Decision Boundary」）。看起来好像工作得不错，因为这条直线表明，随着肿瘤x的增大，y的值也在随之增大；当x的大小达到使得y变成0.5左右的时候，恰好使得更大的肿瘤被标记为恶性，反之则为良性。完美地符合了预测。

然而，考虑如果训练样本中含有一个超大的肿瘤，并且被标记为恶性。如图：
![ml-lr-ng1.png](/images/ml-lr-ng1.png)
这是很自然的一个样本，超大的肿瘤很可能是恶性的，之前训练样本也反映了这个事实。按照线性回归的算法，预测曲线就会变成另外一条更为「平缓」的直线。回忆一下，线性回归的代价函数被定义为样本的预测值与实际值的方差。而这条直线满足这个目标。继续按照之前的算法，即把y大于0.5的样本标记为恶性，那么可以看到预测结果将是很差的，几乎只有最大的两个肿瘤会被预测为恶性。

你可能会问，那为什么不修改「决策边界」呢？为什么这个时候我们不说，当y大于0.3，即为恶性呢？这样我不又可以完美地预测了吗？我的数学水平有限，无法很好地回答这个问题。也许可以通过试验不同的边界值，来得到一个最好的预测？

## 一个新的猜想函数
不管怎么样，公开课里没有讲得这么细。它只告诉你，用线性回归来做分类不是一个好办法。这个时候，我们需要一个新的函数，如下：

$$ h(X) = {1 \over {1 + e^{-\theta^T X}} }$$

正如机器学习里面的许多其他公式一样，第一眼看上去，这个公式可能比较可怕。但如果你仔细地看，会发现指数的值其实就是之前的h(X)的负值。

这个函数有几个奇妙的特性。首先，它可以把任意范围内的实数约束在(0,1)之间。比如，当$$\theta^T X$$趋向无穷大时，h(X)趋向于1；而当前者趋向于负无穷大时，后者趋向于0。其实，当$$\theta^T X$$为7时，h(X)就已经是0.999了；反之，当前者为-7时，后者约为0.001。现在，我们就可以说，当h(X) >= 0.5（此时$$\theta^T X$$为0），我们就预测结果为0。还有一个美妙的特性是，h(X)的值在一定程度上给出了结果为1（在这里，也就是美女）的「概率」。而结果为0的概率自然就是用1减去该概率。

可以看一下S型函数在原结果集上的作用。其中，上图是原结果（θ被初始化为-0.1到0.1的随机数以免溢出），下图则是S型函数作用之后的结果。
![ml-lr-after-sigmoid.png](/images/ml-lr-after-sigmoid.png)

## 代价函数
相比于线性回归，我们同样需要一个新的代价函数。分两种情况，当y为1时，代价函数如下：

$$−log(hθ(x))$$

反之，当y为0时，代价函数如下：

$$−log(1−hθ(x))$$

我不知道这个代价函数是怎么确定的。但是如果我们直接拿来用，会发现这是很合理的。比如当y为1时，h(θ)和代价函数的关系满足类似如下的图：

![ml-lr-cost-y1.png](/images/ml-lr-cost-y1.png)

可以这样理解，如果y是1，并且h(θ)也趋向于1，那么代价就趋向于0；否则代价趋向于无穷大。这就是说，h(θ)和y偏差越大，代价函数的值就越大。当y为0时，情况也是差不多的。

可以注意到，这两个公式其实可以合并为一个公式，如下：

$$ −ylog(h(x))−(1−y)log(1−h(x)) $$

读者可自行推导。

## 梯度下降
在线性回归的时候讲过，梯度下降的问题，其实就是如何逐步找到最优的θ，使得代价J最小。而这个过程，就是求J对于各个θ的「偏微分」，然后通过循环不断地减小J的值。略去推导过程，对于$$ θ_j$$的偏微分为：

$$ {1 \over m} \sum^m_{i=1} (h(x^i)−y^i)x^i_j $$

理论部分大概就是这样，接下来我们讲实战的部分。