---
layout: post
title: 机器学习之「壹零壹」
description: 我不信写成这样你还能看懂！
category: articles
tags: 
---
### 回顾
上一篇我们提到了如何用「梯度下降法」来找到「使得代价函数J最小的参数a」。我不得不用了几个公式，还用到了一点微积分的知识。但如果你对微积分不太了解，也没有什么关系，只需要从几个图形化的表示中获得一点直观上的印象，并记住结论中的几个公式即可。

### 为什么不用「解方程法」？
如果对比「梯度下降法」和「解方程法」，你可能会发现，后者更为简单。为什么还需要「梯度下降法」呢？随着学习的深入，你会慢慢了解到，随着情形变得复杂，「解方程法」的算法的「时间复杂度」其实比「梯度下降法」要高。在数据量和模型本身并不复杂的情况下，两者差别不大。

### 更进一步
不管使用哪种方法，我们至少得到了一致性的结果。那就是J的最优值是37.16（a此时取值4.786）。那么，能不能更好呢？

前面我们一直在假设，房价y和面积X的关系是：

$$ y = aX $$

熟悉代数的同学会发现，这只是「线性」关系的一个特例。更一般的，我们会假设：

$$ y = aX + b $$

所以，我们之前的讨论，都是在假设b为0。因为这是一种符合常识的做法：即假设面积为0的时候，价格也为0。话是这么说没错，但这种简单的模型不一定是最适合预测房价的。换句话说，也许这个简单的「特例模型」可以很好地解释当X为0的时候的情况，但不一定对于其他的X有更好的预测。而且，当X为0的时候这个预测的准确程度本身并没有意义，因为不存在「面积为0的房子」。

前面我们计算J的方式是通过代入已知的面积和房价的值，从而得到一个一元二次方程：

$$ J = {1 \over 6 } (11000a^2 - 105300a + 252225) $$

现在，因为多了一个参数b，J的值现在取决了两个参数a和b了。如果还是像以前那样代入已知数值，我们仍然可以得到一个二元二次方程，只不过这个表达式的项会比较多，这里不再罗列。另外，为了和机器学习领域的说法更为统一，我们现在用h来代表我对房价的「预测」（或称假设hypothesis），而用y表示实际上的房价的值。也就是说，我们「预测」房价h和面积X的关系是：

$$ h = aX + b $$

更进一步地，为避免不必要的混乱，我们这里使用$$\theta_0$$和$$\theta_1$$来代替b和a，所以变成这样：

$$ h = \theta_0 +\theta_1 X $$

因为h是一个关于X的函数，所以更严格地来讲，我们会写成这样：

$$ h(X) = \theta_0 +\theta_1 X $$

这里还要介绍另外一种「上标」表示法，亦即用$$X^{(i)}$$和$$y^{(i)}$$来代表第i个数据。所以，对于每一个数据，上面的h函数可以进一步表示成为：

$$ h(X^{(i)}) = \theta_0 +\theta_1 X^{(i)} $$

对于代价函数J又如何表示呢？如果用h来表示，则为

$$ J = { 1 \over 2 * 3 } ((h(X^{(1)}) - y^{(1)}) ^ 2  + (h(X^{(2)}) - y^{(2)}) ^ 2 + (h(X^{(3)}) - y^{(3)}) ^ 2) $$ 

如果用取和符号西格玛$$\Sigma$$表示，则可以更为简洁地表示成：

$$ J = {1 \over 2 * 3 } \sum_{i=1}^3 (h(X^{(i)})  - y^{(i)}) ^ 2  $$

通过前一篇我们知道，如果J是关于a的一元二次方程，那么计算a的值的关键在于计算J对于a的导数。对于解方程法，我们计算当导数为0时，a的取值。而对于梯度下降法，我们则根据一定的「学习率」按照导数下降。现在，J是关于$$\theta_0$$和$$\theta_1$$的一个二元方程，所以我们需要计算J对于$$\theta_0$$和$$\theta_1$$的「偏微分」。下面的推导会涉及「偏微分」的知识，以及微积分求导的几个「法则」（也就是公式）。如果你觉得头大，直接看结论即可。但是明白推导过程并亲自推导会加深你对这个算法的认识。关于偏微分，我们后面会做更为详细的讲解。

对于上一个公式，我们使用「链式法则」如下：

$$ {\delta J \over \delta \theta} = {\delta J \over \delta h}  * {\delta h \over \delta \theta}  $$

也就是说，为了计算J对于$$\theta_0$$和$$\theta_1$$的偏微分，我们首先计算J对于h的微分，然后再计算h对于θ的偏微分。具体地来讲，
根据幂函数的求导公式，我们去掉平方，得到J对于h的导数，为：

$$ {\delta J \over \delta h} = {1 \over 3} \sum_{i=1}^3 (h(X^{(i)})  - y^{(i)})  $$ 

而h对于θ的偏微分分别为：

$$ {\delta h \over \delta \theta_0} = 1 $$ 

$$ {\delta h \over \delta \theta_1} = X $$ 

所以，根据「链式法则」，我们有：

$$ {\delta J \over \delta \theta_0} =  {1 \over 3} \sum_{i=1}^3 (h(X^{(i)})  - y^{(i)}) $$ 

$$ {\delta J \over \delta \theta_1} =  {1 \over 3} \sum_{i=1}^3 (h(X^{(i)})  - y^{(i)}) * X^{(i)} $$ 

按照「解方程法」，我们只需解下面的方程：

$$ \sum_{i=1}^3 (h(X^{(i)})  - y^{(i)}) = 0 $$

$$ \sum_{i=1}^3 (h(X^{(i)})  - y^{(i)}) * X^{(i)} = 0 $$ 

如果代入X和y的值，这两个方程就变成了一组二元一次方程组，一个初中生都可以解决的问题了，如下：

$$ 3\theta_0 + 180\theta_1 = 865 $$

$$ 180\theta_0 + 11000\theta_1 = 52650 $$

你可以自己试着解一下，可以得到$$\theta_0$$和$$\theta_1$$的值分别为63.3和3.75。如果你此时计算J的值（即方差），可以发现结果低至0.69！这相对于37.16是一个相当大的进步。如果这个时候我们看一下图，可以发现我的模型几乎完美地预测了房价（也就是所有的点几乎都落在了直线上）。

![ml_normalEqn.png](/images/ml_normalEqn.png)

类似的，如果我们用偏微分去「梯度下降」J，也可以得到类似的结果。公式如下：

$$ \theta_0 := \theta_0 - \alpha * {\delta J \over \delta \theta_0}  $$ 

$$ \theta_1 := \theta_0 - \alpha * {\delta J \over \delta \theta_1}  $$ 

当然，在我实际写代码的过程中，我发现$$\alpha$$的取值很难：太小则收敛太慢；稍大则导致发散。最终导致即使循环了几万次，仍不能使J足够收敛。对于一个这么少的数据的简单模型，这让我非常吃惊。不过，后来我才发现，是需要做「Feature Scaling」。简单来讲，就是对X进行「缩放」。即，我们不再用「每平方米」作为单位，而是「每十平方米」。这么做了之后，如果我们把J和$$\theta_0$$和$$\theta_1$$的关系画一个三维的图，可以发现在$$\theta_0$$和$$\theta_1$$两个维度上有着类似的斜率（导数），因此可以足够快地收敛。

![ml_gradientDescentRtl3D.png](/images/ml_gradientDescentRtl3D.png)

这篇写得比较长，也花了我好几天的时间。其中有几天我一直在补微积分特别是偏微分的知识。当我终于想明白了一个比较复杂的数学问题的时候，体验到一种似乎是「高级的情趣」。不过…我猜，写到这里，已经没有人能看懂我在说什么了，即使你有耐心看到现在——除非你本来就懂。