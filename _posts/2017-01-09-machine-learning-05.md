---
layout: post
title: 机器学习之「壹零壹」
description: 
category: articles
tags: 
---
### 回顾
上一篇我们提到了如何用「梯度下降法」来找到「使得代价函数J最小的参数a」。我不得不用了几个公式，还用到了一点微积分的知识。但如果你对微积分不太了解，也没有什么关系，只需要从几个图形化的表示中获得一点直观上的印象，并记住结论中的几个公式即可。

### 为什么不用「解方程法」？
如果对比「梯度下降法」和「解方程法」，你可能会发现，后者更为简单。为什么还需要「梯度下降法」呢？随着学习的深入，你会慢慢了解到，随着情形变得复杂，「解方程法」的算法的「时间复杂度」其实比「梯度下降法」要高。在数据量和模型本身并不复杂的情况下，两者差别不大。

### 更进一步
不管使用哪种方法，我们至少得到了一致性的结果。那就是J的最优值是37.16（a此时取值4.786）。那么，能不能更好呢？

前面我们一直在假设，房价y和面积X的关系是：

$$ y = aX $$

熟悉代数的同学会发现，这只是「线性」关系的一个特例。更一般的，我们会假设：

$$ y = aX + b $$

所以，我们之前的讨论，都是在假设b为0。因为这是一种符合常识的做法：即假设面积为0的时候，价格也为0。话是这么说没错，但这种简单的模型不一定是最适合预测房价的。换句话说，也许这个简单的「特例模型」可以很好地解释当X为0的时候的情况，但不一定对于其他的X有更好的预测。而且，当X为0的时候这个预测的准确程度本身并没有意义，因为不存在「面积为0的房子」。

前面我们计算J的方式是通过代入已知的面积和房价的值，从而得到一个一元二次方程：

$$ J = { 11000a^2 - 105300a + 252225 \over 6 } $$

现在，因为多了一个参数b，J的值现在取决了两个参数a和b了。如果还是像以前那样代入已知数值，我们仍然可以得到一个二元二次方程，但是这个表达式的项会比较多。所以，这次，我们来换一种方式。另外，为了和机器学习领域的说法更为统一，我们现在用h来代表我对房价的「预测」（或称假设hypothesis），而用y表示实际上的房价的值。也就是说，我们「预测」房价h和面积X的关系是：

$$ h = aX + b $$

为了和业界的术语更加统一，和避免不必要的混乱，我们这里使用$$\theta_0$$和$$\theta_1$$来代替b和a，所以变成这样：

$$ h = \theta_0 +\theta_1 X $$

因为h是一个关于X的函数，所以更严格地来讲，我们会写成这样：

$$ h(X) = \theta_0 +\theta_1 X $$

这里还要介绍另外一种「上标」表示法，亦即用$$X^{(i)}$$和$$y^{(i)}$$来代表第i个数据。所以，对于每一个数据，上面的h函数可以进一步表示成为：

$$ h(X^{(i)}) = \theta_0 +\theta_1 X^{(i)} $$

对于代价函数J又如何表示呢？如果用h来表示，则为

$$ J = { 1 \over 2 * 3 } (h(X^{(1)}) - y^{(1)}) ^ 2  + (h(X^{(2)}) - y^{(2)}) ^ 2 + (h(X^{(3)}) - y^{(3)}) ^ 2 $$ 

如果用取和符号西格玛$$\Sigma$$表示，则可以更为简洁地表示成：

$$ J = {1 \over 2 * 3 } \sum_{i=1}^3 (h(X^{(i)})  - y^{(i)}) ^ 2  $$

如果用「解方程法」来求解，那么还是求当J为0时，$$\theta_0$$和$$\theta_1$$（也就是之前的b和a）的取值。但是，这里我们打算用「梯度下降法」。这是因为：

* 虽然从算式的表达上，梯度下降法看起来似乎更为复杂，但是当你用图形表示的时候，会更为直观。
* 因为梯度下降法对于复杂的模型更容易扩展（scale），所以是更为普遍采用的机器学习算法。

通过前一篇我们知道，如果J是关于a的一元二次方程，那么计算a的值的关键在于对a进行「求导」，然后用一定的学习率$$\alpha$$来逐步「优化」J的值以至收敛。类似地，对于一个二元方程，我们要求的，则是计算$$\theta_0$$和$$\theta_1$$对于J的「偏微分」。同样的，下面的推导过程仍然会涉及到微积分的知识；特别的，会要求你知道偏微分是怎么回事，以及微积分求导的几个「法则」（也就是公式）。如果你觉得头大，直接看结论即可。但是明白推导过程并亲自推导会加深你对这个算法的认识。关于偏微分，我们后面会做更为详细的讲解。

我们可以代入X和y，然后根据二元二次方程来计算J对于$$\theta_0$$和$$\theta_1$$的偏微分，但是因为那样多项式展开看起来比较复杂。所以我们打算使用「链式法则」：

$$ {\delta J \over \delta \theta} = {\delta J \over \delta h}  * {\delta h \over \delta \theta}  $$

也就是说，为了计算J对于$$\theta_0$$和$$\theta_1$$的偏微分，我们首先计算J对于h的微分，然后再计算h对于θ的偏微分。具体地来讲，
根据幂函数的求导公式，我们去掉平方，得到J对于h的导数，为：

$$ {\delta J \over \delta h} = {1 \over 3} \sum_{i=1}^3 (h(X^{(i)})  - y^{(i)})  $$ 

而h对于θ的偏微分分别为：

$$ {\delta h \over \delta \theta_0} = 1 $$ 

$$ {\delta h \over \delta \theta_1} = X $$ 

所以，根据「链式法则」，我们有：

$$ {\delta J \over \delta \theta_0} =  {1 \over 3} \sum_{i=1}^3 (h(X^{(i)})  - y^{(i)}) $$ 

$$ {\delta J \over \delta \theta_1} =  {1 \over 3} \sum_{i=1}^3 (h(X^{(i)})  - y^{(i)}) * X^{(i)} $$ 

然后就是像之前一样，循环更新$$\theta_0$$和$$\theta_1$$的值：

$$ \theta_0 := \theta_0 - \alpha * {\delta J \over \delta \theta_0}  $$ 

$$ \theta_1 := \theta_0 - \alpha * {\delta J \over \delta \theta_1}   $$ 

如果alpha取值为xxx，那么我们可以得到：

$$ a = x; b = xx;$$ 

此时J的值为xxx，比37.16又好了不少，太好了！可是……我猜已经没有人能看懂我在说什么了，即使你有耐心看到现在——除非你本来就懂。