---
layout: post
title: 机器学习之「壹零零」
description: 梯度下降法
category: articles
tags: 
---
上次我们讲了用「方程求解法」来算出a的值，以使得J最小：

`J = (11000 * (a^2) - 105300 * a + 252225) / 6`

今天讲第二种办法

### 梯度下降法
这个方法之所以叫「梯度下降法」是因为其实现过程就像「下楼梯」一样，一次一小步，以达到终点（即找到J的最小值）。基本的方法如下，我们可以任意初始化a的值。不失一般性，假设a为0。接下来，我们就沿着这条曲线「以细微的步伐下降」，如图中红叉所示，直到最低点。

![ml_gradientDescent.png](/images/ml_gradientDescent.png)

听着好像很容易，可是怎么做呢？恐怕这里我们还是要用到微积分的知识。

> 如果你不熟悉微积分，那你也可以跳过推导步骤直接看结论。实际上，机器学习的很多理论的数学推导过程是很复杂的，这里所列举的只是最简单的情况。你其实也没有必要知道推导的过程，只需要套用公式，就可以把机器学习在工作中运用得很好。当然，不仅仅是套用公式，还要考虑整个系统的设计，例如如何处理出错的情况等等。这又是新的话题了。

具体地来讲，首先，我们要对文章开头的那个公式求导。根据上一篇文章，我们知道，对于一个任意的一元二次方程，在x点的导数为

`2ax + b`

这也是为什么当导数为0时，x为-b/2a。将第一个等式中的常数代入，我们有（假设倒数为d）：

`d = (11000*2)*a - 105300 = 22000*a - 105300`

然后，就是选择一个足够小的「步伐」（专有名词叫Learning Rate，即学习曲率，在这里用alpha表示），开始「不断地」下降。具体地来说，就是对于a，不断地（往往需要几千几万次）更新a的值：

`a := a - alpha * d`

如果选择alpha为0.00001，那么大约在迭代不到100次之后，我们就可以得到和用「方程求解法」一样的答案，即4.786。如果继续增加迭代的次数，a的值保持不变。因为已经到了「最优点」，即图的底步，其导数（斜率）为0。


你可能要问，如果a的初始值不为0呢？比如落在曲线的右侧，如初始值为10？这个时候因为其导数是正的，所以a的值会逐渐减小，你可以看到类似如下效果：
![ml_gradientDescentRtl.png](/images/ml_gradientDescentRtl.png)

不知道你有没有发现。一开始的「步伐」似乎是很大的，但是后来步伐越来越小，直至为0。这是因为，虽然我们的学习曲率alpha是固定的，但是导数（斜率）却是随着逐渐靠近最优点而逐步减小的。

或者你会说，好吧，微积分…太抽象了。我们必须要用微积分吗？从这个图来看，如果我们要求解「a的值在什么时候可以让J最小」，难道不能这么做吗？

1. 初始化a为0
2. 每次对a增加固定的值，例如0.001，并计算J的值
3. 查看J的值是变大还是减小
4. 如果J的值一直在变小，那么一直循环。否则我们认为已经找到了J的最小值，至少是近似的最小值。

这个过程可以表示成下图：
![ml_gradientDescentSimple.png](/images/ml_gradientDescentSimple.png)

这其实是个很好的问题！因为这个办法看着很直观，一开始我也有点疑惑。但我后来才想明白，这种做法过于简单，会有几个限制：

> 假如a的值不是初始为0，而是，比如10呢？或者在解的附近，如4.8？

这个时候你怎么知道一开始是要增加还是减小a的值？当然，一种办法是先试着增加，如果发现J也变大，那就减小。另一种极端的情况是a的初始值刚好在最优点附近，那么你会发现无论增加或减少a都会导致J变大。所以，虽然这是一种行得通的做法，在「直觉」上也更容易理解，但在计算上反而显得复杂。因为直接用导数就不会有这些问题，不需要判断这些情况。

另外一种办法是，像前面那样把图画出来，不就一目了然了吗？我甚至可以在4和5之间直接寻找J的最小值。对于这种简单的模型，画图（Plotting）是一种很好的办法。但，这并不总是可能的。因为我们最多只能在二维平面上形象地反应三维空间里里的图，再多维我们就没法画了，也很难理解。所以画图可以帮助我们在一个低维的层次上去理解一个问题，高维的情形就就只能从低维推导的数学模型得出了。

> 可能「收敛」不够快

所谓「收敛」，就是让J的值变小的过程，也就是说多快可以达到最优解。我们看到，用微积分的方法，一开始收敛是很快的，后来虽然变慢，但我们已经接近最优解，所以可以接受。但这种「直觉」上简单的做法收敛速度是不变的。

### 关于学习曲率alpha
前面我们取了一个很小的alpha值，0.00001。这个值的取法很有讲究，如果太小，则容易收敛太慢。但如果太大，则可能会因为收敛太快（步子太大）导致「跳过」最优解。

例如，如果我们对alpha的值乘以7，取0.00007，那么就会看到，有几个红叉直接「跳过」了最低点：

![ml_gradientDescentAlpha1.png](/images/ml_gradientDescentAlpha1.png)

或者，如果继续稍稍增大alpha的值，到0.0001，那么J的值甚至无法收敛，反而「发散」了：

![ml_gradientDescentAlpha2.png](/images/ml_gradientDescentAlpha2.png)