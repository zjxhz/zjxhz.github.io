---
layout: post
title: 机器学习之「壹零零壹」
description:  参数调整和过度拟合
category: articles
tags: 
---

上一篇说到，如果单一地增加某个属性如面积的幂次，可以得到对于现有数据偏差更小的预测函数。然而，这个函数有所谓的「过度拟合」问题，也就是，它对未知数据的预测较差。那么这一篇来进一步解释如何解决该问题。

作为回顾，从上一篇的最后一张图可以看到，价格预测在面积接近200平方的时候出现了「断崖式」下跌，并使得价格出现负数。

![](/images/ml_hg8.png)

有一点需要说明的是，这个图是通过「解方程法」得出的。如果用一般的梯度下降法，那么情形会稍好，得到类似如下的图：

![](/images/ml_gdReal.png)

这个图稍显合理，但如果看其对第一套房的预测，仍然有很大的偏差（虽然还没有出现负数）。

| 序号 | 面积 | 实际价格 | 预测价格 | 差价 | 偏差率
| --- | ---- | ----    | ------ | ---   | ----
| 1   | 432 | 1950 | 142233 | 140283 | 7194%
| 2   | 66 | 380 | 372 | 8 | 2.0%
| 3   | 68 | 382 | 385 | 3 | 0.8%

那么，如果这个时候我们对参数进行一定的惩罚（regularize），即使得任何参数过大都会导致偏差函数增大，我们有如下公式：

$$ J = {1 \over 2m } \sum_{i=1}^m (h(X^{(i)})  - y^{(i)}) ^ 2  + {\lambda \over 2m} \sum_{j=1}^n \theta_j^2 $$

可以看到，这个公式的后半部分将参数本身的值计入了代价函数的计算，从而达到了对参数进行「惩罚」的目的。这时，重新画一张图：

![](/images/ml_gdRealReg.png)

可以看到，预测曲线比较平滑，基本上退化到了线性的关系。这是因为，在同一个区内，房价和面积基本上就是一个线性的关系。再看这个图对价格的预测：

| 序号 | 面积 | 实际价格 | 预测价格 | 差价 | 偏差率
| --- | ---- | ----    | ------ | ---   | ----
| 1   | 432 | 1950 | 4847 | 2897 | 148.6%
| 2   | 66 | 380 | 371 | 9 | 2.4%
| 3   | 68 | 382 | 382 | 0 | 0.8%

写到这里，我们发现，这么来回折腾似乎是「然并卵」。简单地增加高次幂似乎对现有的数据预测更好了，但是对于未知数据却非常差劲。如果我们对参数规范化，那么又退化成基本线性的关系，预测的价格也看不出来比线性的更好。那么，还有什么可以改进的地方吗？