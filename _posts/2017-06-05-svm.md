---
layout: post
title: 支持向量机
description: 
category: articles
tags: 
---
支持向量机，Support Vector Machine，一个奇怪的名字。在我刚听到这个名字的时候，我以为这是一种为了机器学习而建造的机器，即某种具备特殊能力的计算机。而事实上，我发现，这只是一种有个非常差劲的名字的「算法」。我试着去搜索了这个名字的来由，但仍不得要领 —— 即使是在实现了这个算法之后。

__算法__

还是给美女分类作为例子，该算法直观上的理解，大致是这样：

1. 给一张人脸打分。得到两个分数，是美女的分数X，和不是美女的分数Y。 
2. 该算法会「希望」正确的那个分数比不正确的那个更高，而且要高出一个指定的值，例如1。
3. 如果条件2不满足，那么我们就说这个打分系统不好，并且开始优化，直到找到一组最优的参数。

举个例子，对于人脸A，属于美女，该算法打出两个分数，分别是美女分0.8，非美女分-0.4。这个时候我们说该算法对于人脸A是理想的，因为正确分减去不正确的分为1.2，满足大于1的条件，此时「损失值」为0。但是如果美女分只有0.4，那么差额只有0.8，我们就累计了「损失值」0.2（即1 - 0.8）。更甚者，如果反过来，美女分是-0.4，而非美女分是0.4，那么损失值为1.8（即1 - (-0.4 - 0.4)）。

![ml_svm_happy.png](/images/ml_svm_happy.png)
![ml_svm_unhappy.png](/images/ml_svm_unhappy.png)

用数学来表示，就是：

$$ L_i = max(0, s_j - s_y + 1) $$

其中$$Li$$表示对于某个特定的人脸的损失（Loss）值。$$s_j$$为不正确的分类的分数，$$s_y$$为正确的分类的分数。所以：

$$ s_j = w^T_j x_i$$

这个公式可以推广到有多个分类的情况。而且，对于二元分类，其实有个更简单地公式，留到以后再讲。代入则有：

$$ L_i = max(0, w^T_j x_i - w^T_y x_i + 1) $$

总损失则当然就是：

$$ L = {1 \over N} \sum L_i $$

当然了，这里还有个问题，就是需要正则化，否则就会有无穷多个解。变成这样：

$$ L = {1 \over N} \sum_i L_i + \lambda R(W) $$

其中：

$$ R(W) = \sum_k \sum_l W^2_{k,l} $$

全部展开，则有：

$$ L = {1 \over N} \sum_i max(0, w^T_j x_i - w^T_y x_i + 1) + \lambda \sum_k \sum_l W^2_{k,l} $$

损失函数有了，接下来就是计算dW，即计算参数W对于损失函数L的偏微分，即梯度。通过用微积分求导，对于单个损失值，我们有：

$$ \Delta_{w_y} L_i = -1(w^T_j x_i - w^T_y x_i + 1 > 0)x_i $$

其中的1表示如果后面的条件满足，则为1，否则为0。注意，这个是针对正确的那个分类的，对于不正确的分类，取正值即可。

$$ \Delta_{w_j} L_i = 1 (w^T_j x_i - w^T_y x_i + 1 > 0)x_i $$

我们最终想要的是dW关于L的偏微分，而不是对单个$$L_i$$的。要计算前者，简单将各项相加即可。更详细的内容，参考斯坦福大学的cs231的[Course Notes](http://cs231n.github.io/)中的SVM部分。